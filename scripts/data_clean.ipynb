{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-S7PWWrg4-p"
      },
      "source": [
        "# Data cleaning sandbox\n",
        "\n",
        "This is a quick sandbox to get an unbiased estimate of the effect of different levels of data cleaning on model performance\n",
        "Not intended to be used later on in practice!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n99ELxF8-H4Y"
      },
      "source": [
        "Now, we can (hopefully) import all the necessary libraries. If this should not be the case, please install the packages you do not have."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xy3bELediR7O",
        "outputId": "bdab13eb-59fd-4b36-869b-657114f8602f"
      },
      "outputs": [],
      "source": [
        "import earthnet as en\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from os.path import join\n",
        "import statistics as st\n",
        "import pickle\n",
        "import glob\n",
        "from numpy import genfromtxt\n",
        "from numpy.random import shuffle\n",
        "import re\n",
        "import sys\n",
        "from random import seed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Oto\\Documents\\GitHub\\drought_impact_forecasting\n"
          ]
        }
      ],
      "source": [
        "sys.path.append(os.getcwd())\n",
        "os.chdir(join(os.getcwd(), \"..\"))\n",
        "print(os.getcwd()) # Should be top drought_impact_forecasting folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "6cCUbyD0o9Id"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(23904, 5)\n",
            "23904\n",
            "(23904,)\n",
            "(23904, 6)\n"
          ]
        }
      ],
      "source": [
        "baseline_scores = genfromtxt(join(os.getcwd(), \"Data\", \"scores_last_frame.csv\"), delimiter=',')\n",
        "with open(join(os.getcwd(), \"Data\", \"last_frame_data_paths.pkl\"),'rb') as f:\n",
        "    old_train_paths = pickle.load(f)\n",
        "\n",
        "print(baseline_scores.shape)\n",
        "print(len(old_train_paths))\n",
        "\n",
        "# Glue together baseline scores and paths\n",
        "path_arr = np.array(old_train_paths)\n",
        "print(path_arr.shape)\n",
        "scores = np.append(baseline_scores, np.zeros([len(old_train_paths),1]), axis=1)\n",
        "scores[:,5] = range(0, len(old_train_paths))\n",
        "print(scores.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'numpy.float64'>\n",
            "[[0.22392087 0.36124287 0.37945882 0.25867528 0.29122114 0.        ]]\n",
            "[[2.13312454e-01 1.04168812e-01 2.60996524e-01 1.70339861e-01\n",
            "  1.66737208e-01 2.39030000e+04]]\n",
            "[0.0000e+00 1.0000e+00 2.0000e+00 ... 2.3901e+04 2.3902e+04 2.3903e+04]\n"
          ]
        }
      ],
      "source": [
        "print(type(scores[0,4]))\n",
        "print(scores[:1,:])\n",
        "print(scores[-1:,:])\n",
        "print(scores[:,5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1.37477925e-01 5.07578757e-04 2.75217608e-01 1.05062470e-01\n",
            "  2.00948160e-03 7.69000000e+03]]\n",
            "[[2.11230905e-01            nan 5.17031017e-01 2.36936472e-01\n",
            "             nan 2.21650000e+04]]\n",
            "(23904, 6)\n",
            "[[2.14623642e-01            nan 4.10139015e-01 2.06978211e-01\n",
            "             nan 2.08410000e+04]\n",
            " [2.29622759e-01            nan 6.05806542e-01 2.50108325e-01\n",
            "             nan 7.97000000e+02]\n",
            " [2.11230905e-01            nan 5.17031017e-01 2.36936472e-01\n",
            "             nan 2.21650000e+04]]\n"
          ]
        }
      ],
      "source": [
        "sorted_scores = scores[scores[:, 4].argsort()]\n",
        "np.savetxt(\"Data/scores_last_frame_sorted.csv\", sorted_scores, delimiter=\",\")\n",
        "print(sorted_scores[:1])\n",
        "print(sorted_scores[-1:])\n",
        "print(sorted_scores.shape)\n",
        "\n",
        "print(sorted_scores[np.isnan(sorted_scores[:, 4])])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3\n",
            "1046\n",
            "22855\n"
          ]
        }
      ],
      "source": [
        "threshold = 0.1\n",
        "nan_samples = sorted_scores[np.isnan(sorted_scores[:, 4])]\n",
        "print(nan_samples.shape[0])\n",
        "bad_samples = sorted_scores[sorted_scores[:, 4]<threshold]\n",
        "print(bad_samples.shape[0])\n",
        "good_samples = sorted_scores[sorted_scores[:, 4]>=threshold]\n",
        "print(good_samples.shape[0])\n",
        "# Should add up to 23904"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "21355\n",
            "500\n",
            "1000\n"
          ]
        }
      ],
      "source": [
        "# Split 'Good' dataset into train/val_1,val_2\n",
        "seed(1)\n",
        "shuffle(good_samples)\n",
        "val_2 = good_samples[:1000]\n",
        "val_1 = good_samples[1000:1500]\n",
        "train = good_samples[1500:]\n",
        "\n",
        "train_data = path_arr[list(train[:,5].astype(int))].tolist()\n",
        "print(len(train_data))\n",
        "val_1_data = path_arr[list(val_1[:,5].astype(int))].tolist()\n",
        "print(len(val_1_data))\n",
        "val_2_data = path_arr[list(val_2[:,5].astype(int))].tolist()\n",
        "print(len(val_2_data))\n",
        "\n",
        "bad_data = path_arr[list(bad_samples[:,5].astype(int))].tolist()\n",
        "nan_data = path_arr[list(nan_samples[:,5].astype(int))].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save 'Good' dataset\n",
        "cur_dir = 'all_data_1'\n",
        "if not os.path.exists(join('Data', cur_dir)):\n",
        "    os.mkdir(join('Data', cur_dir))\n",
        "with open(join(os.getcwd(), \"Data\", cur_dir, \"train_data_paths.pkl\"), \"wb\") as fp:\n",
        "    pickle.dump(train_data, fp)\n",
        "with open(join(os.getcwd(), \"Data\", cur_dir, \"val_1_data_paths.pkl\"), \"wb\") as fp:\n",
        "    pickle.dump(val_1_data, fp)\n",
        "with open(join(os.getcwd(), \"Data\", cur_dir, \"val_2_data_paths.pkl\"), \"wb\") as fp:\n",
        "    pickle.dump(val_2_data, fp)\n",
        "\n",
        "# Save 'OK' dataset\n",
        "cur_dir = 'all_data_2'\n",
        "if not os.path.exists(join('Data', cur_dir)):\n",
        "    os.mkdir(join('Data', cur_dir))\n",
        "with open(join(os.getcwd(), \"Data\", cur_dir, \"train_data_paths.pkl\"), \"wb\") as fp:\n",
        "    pickle.dump(train_data + bad_data, fp)\n",
        "with open(join(os.getcwd(), \"Data\", cur_dir, \"val_1_data_paths.pkl\"), \"wb\") as fp:\n",
        "    pickle.dump(val_1_data, fp)\n",
        "with open(join(os.getcwd(), \"Data\", cur_dir, \"val_2_data_paths.pkl\"), \"wb\") as fp:\n",
        "    pickle.dump(val_2_data, fp)\n",
        "\n",
        "# Save 'Bad' dataset\n",
        "cur_dir = 'all_data_3'\n",
        "if not os.path.exists(join('Data', cur_dir)):\n",
        "    os.mkdir(join('Data', cur_dir))\n",
        "with open(join(os.getcwd(), \"Data\", cur_dir, \"train_data_paths.pkl\"), \"wb\") as fp:\n",
        "    pickle.dump(train_data + bad_data + nan_data, fp)\n",
        "with open(join(os.getcwd(), \"Data\", cur_dir, \"val_1_data_paths.pkl\"), \"wb\") as fp:\n",
        "    pickle.dump(val_1_data, fp)\n",
        "with open(join(os.getcwd(), \"Data\", cur_dir, \"val_2_data_paths.pkl\"), \"wb\") as fp:\n",
        "    pickle.dump(val_2_data, fp)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save 'Good' dataset\n",
        "cur_dir = 'all_data_4'\n",
        "if not os.path.exists(join('Data', cur_dir)):\n",
        "    os.mkdir(join('Data', cur_dir))\n",
        "with open(join(os.getcwd(), \"Data\", cur_dir, \"train_data_paths.pkl\"), \"wb\") as fp:\n",
        "    pickle.dump(train_data, fp)\n",
        "with open(join(os.getcwd(), \"Data\", cur_dir, \"val_1_data_paths.pkl\"), \"wb\") as fp:\n",
        "    pickle.dump(val_1_data, fp)\n",
        "with open(join(os.getcwd(), \"Data\", cur_dir, \"val_2_data_paths.pkl\"), \"wb\") as fp:\n",
        "    pickle.dump(val_2_data, fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'list'>\n"
          ]
        }
      ],
      "source": [
        "# test reloading works\n",
        "with open(os.path.join(os.getcwd(), \"Data\", cur_dir, \"train_data_paths.pkl\"),'rb') as f:\n",
        "    loaded_paths = pickle.load(f)\n",
        "print(type(loaded_paths))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "ModelDemo.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "035801ae1542a13dec20f9e5dfba70b1f0c9b17e94c66e01382d120ab9da26bf"
    },
    "kernelspec": {
      "display_name": "Python 3.8.2 64-bit ('pai_proj': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
